{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest With Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6545454545454545\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      1.00      1.00        18\n",
      "        7762       0.60      0.63      0.61        68\n",
      "        7763       0.43      0.23      0.30        13\n",
      "        7765       0.93      0.76      0.84        50\n",
      "        7771       0.33      0.10      0.15        10\n",
      "        7800       0.75      0.88      0.81        24\n",
      "        7801       0.50      0.86      0.63        14\n",
      "        7803       1.00      0.17      0.29         6\n",
      "        7804       0.56      0.59      0.58        68\n",
      "        7821       0.80      1.00      0.89         4\n",
      "        7825       0.29      0.33      0.31         6\n",
      "        7830       0.60      0.80      0.69        15\n",
      "        7833       0.80      0.40      0.53        20\n",
      "       57663       0.50      0.90      0.64        10\n",
      "       57671       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.65       330\n",
      "   macro avg       0.66      0.64      0.61       330\n",
      "weighted avg       0.67      0.65      0.64       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Cargar el dataset_\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Crear una nueva columna uniendo company_tid y establishment_id\n",
    "df['company_establishment'] = df['company_tid'].astype(str) + '_' + df['establishment_id'].astype(str)\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_establishment', 'unit_total', 'classification']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_establishment', 'classification']),\n",
    "        ('num', 'passthrough', ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo de Random Forest\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar los registros que no fueron clasificados correctamente\n",
    "misclassified_mask = y_test != y_pred\n",
    "misclassified = X_test[misclassified_mask].copy()\n",
    "misclassified['actual'] = y_test[misclassified_mask].values\n",
    "misclassified['predicted'] = y_pred[misclassified_mask]\n",
    "\n",
    "# Guardar los registros que no fueron clasificados correctamente en un archivo CSV\n",
    "misclassified.to_csv('misclassified_records.csv', index=False)\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Without Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6363636363636364\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      1.00      1.00        18\n",
      "        7762       0.64      0.65      0.64        68\n",
      "        7763       0.40      0.31      0.35        13\n",
      "        7765       0.93      0.78      0.85        50\n",
      "        7771       0.00      0.00      0.00        10\n",
      "        7800       0.65      0.83      0.73        24\n",
      "        7801       0.47      0.57      0.52        14\n",
      "        7803       1.00      0.17      0.29         6\n",
      "        7804       0.59      0.60      0.59        68\n",
      "        7821       0.50      0.50      0.50         4\n",
      "        7825       0.00      0.00      0.00         6\n",
      "        7830       0.56      0.67      0.61        15\n",
      "        7833       0.67      0.50      0.57        20\n",
      "       57663       0.50      0.90      0.64        10\n",
      "       57671       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.64       330\n",
      "   macro avg       0.58      0.57      0.54       330\n",
      "weighted avg       0.64      0.64      0.63       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid']),\n",
    "        ('num', 'passthrough', ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo de Random Forest\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(cls) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost WITHOUT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5969696969696969\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      0.94      0.97        18\n",
      "        7762       0.57      0.59      0.58        68\n",
      "        7763       0.50      0.38      0.43        13\n",
      "        7765       0.85      0.70      0.77        50\n",
      "        7771       0.00      0.00      0.00        10\n",
      "        7800       0.55      0.71      0.62        24\n",
      "        7801       0.53      0.71      0.61        14\n",
      "        7803       0.00      0.00      0.00         6\n",
      "        7804       0.51      0.56      0.53        68\n",
      "        7821       0.67      0.50      0.57         4\n",
      "        7825       0.12      0.17      0.14         6\n",
      "        7830       0.59      0.67      0.62        15\n",
      "        7833       0.71      0.50      0.59        20\n",
      "       57663       0.44      0.80      0.57        10\n",
      "       57671       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.60       330\n",
      "   macro avg       0.54      0.55      0.53       330\n",
      "weighted avg       0.59      0.60      0.59       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanc\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\juanc\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\juanc\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Codificar la columna de la variable objetivo\n",
    "label_encoder_acc = LabelEncoder()\n",
    "df['acc_classification'] = label_encoder_acc.fit_transform(df['acc_classification'])\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid']),\n",
    "        ('num', 'passthrough', ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo XGBoost\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(label_encoder_acc.inverse_transform([cls])[0]) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST WITH CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6151515151515151\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      0.94      0.97        18\n",
      "        7762       0.59      0.57      0.58        68\n",
      "        7763       0.44      0.31      0.36        13\n",
      "        7765       0.83      0.76      0.79        50\n",
      "        7771       0.67      0.20      0.31        10\n",
      "        7800       0.55      0.75      0.63        24\n",
      "        7801       0.60      0.64      0.62        14\n",
      "        7803       1.00      0.67      0.80         6\n",
      "        7804       0.54      0.59      0.56        68\n",
      "        7821       0.50      0.50      0.50         4\n",
      "        7825       0.12      0.17      0.14         6\n",
      "        7830       0.50      0.53      0.52        15\n",
      "        7833       0.62      0.40      0.48        20\n",
      "       57663       0.53      0.90      0.67        10\n",
      "       57671       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.62       330\n",
      "   macro avg       0.62      0.60      0.59       330\n",
      "weighted avg       0.63      0.62      0.61       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Codificar la columna de la variable objetivo\n",
    "label_encoder_acc = LabelEncoder()\n",
    "df['acc_classification'] = label_encoder_acc.fit_transform(df['acc_classification'])\n",
    "\n",
    "# Codificar la columna classification\n",
    "label_encoder_classification = LabelEncoder()\n",
    "df['classification'] = label_encoder_classification.fit_transform(df['classification'])\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'classification', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid', 'classification']),\n",
    "        ('num', 'passthrough', ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo XGBoost\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb.XGBClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(label_encoder_acc.inverse_transform([cls])[0]) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM WITH CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5878787878787879\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       0.90      1.00      0.95        18\n",
      "        7762       0.60      0.82      0.70        68\n",
      "        7763       0.38      0.62      0.47        13\n",
      "        7765       0.72      0.36      0.48        50\n",
      "        7771       1.00      0.10      0.18        10\n",
      "        7800       0.35      0.88      0.50        24\n",
      "        7801       0.56      0.36      0.43        14\n",
      "        7803       1.00      0.67      0.80         6\n",
      "        7804       0.69      0.40      0.50        68\n",
      "        7821       0.00      0.00      0.00         4\n",
      "        7825       0.00      0.00      0.00         6\n",
      "        7830       0.77      0.67      0.71        15\n",
      "        7833       0.63      0.60      0.62        20\n",
      "       57663       0.53      1.00      0.69        10\n",
      "       57671       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.59       330\n",
      "   macro avg       0.61      0.56      0.54       330\n",
      "weighted avg       0.64      0.59      0.57       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Codificar la columna de la variable objetivo\n",
    "label_encoder_acc = LabelEncoder()\n",
    "df['acc_classification'] = label_encoder_acc.fit_transform(df['acc_classification'])\n",
    "\n",
    "# Codificar la columna classification\n",
    "label_encoder_classification = LabelEncoder()\n",
    "df['classification'] = label_encoder_classification.fit_transform(df['classification'])\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'classification', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder y StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid', 'classification']),\n",
    "        ('num', StandardScaler(), ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo SVM\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(kernel='linear', random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(label_encoder_acc.inverse_transform([cls])[0]) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5757575757575758\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      1.00      1.00        18\n",
      "        7762       0.58      0.82      0.68        68\n",
      "        7763       0.38      0.62      0.47        13\n",
      "        7765       0.70      0.38      0.49        50\n",
      "        7771       1.00      0.10      0.18        10\n",
      "        7800       0.34      0.83      0.49        24\n",
      "        7801       0.56      0.36      0.43        14\n",
      "        7803       1.00      0.67      0.80         6\n",
      "        7804       0.60      0.46      0.52        68\n",
      "        7821       0.00      0.00      0.00         4\n",
      "        7825       0.00      0.00      0.00         6\n",
      "        7830       0.75      0.40      0.52        15\n",
      "        7833       0.80      0.40      0.53        20\n",
      "       57663       0.53      1.00      0.69        10\n",
      "       57671       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.58       330\n",
      "   macro avg       0.62      0.54      0.52       330\n",
      "weighted avg       0.63      0.58      0.56       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Codificar la columna de la variable objetivo\n",
    "label_encoder_acc = LabelEncoder()\n",
    "df['acc_classification'] = label_encoder_acc.fit_transform(df['acc_classification'])\n",
    "\n",
    "# Codificar la columna classification\n",
    "label_encoder_classification = LabelEncoder()\n",
    "df['classification'] = label_encoder_classification.fit_transform(df['classification'])\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'classification', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder y StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid', 'classification']),\n",
    "        ('num', StandardScaler(), ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo Logistic Regression\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(label_encoder_acc.inverse_transform([cls])[0]) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 172\n",
      "[LightGBM] [Info] Number of data points in the train set: 1316, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score -2.962844\n",
      "[LightGBM] [Info] Start training from score -1.668923\n",
      "[LightGBM] [Info] Start training from score -3.250526\n",
      "[LightGBM] [Info] Start training from score -1.972866\n",
      "[LightGBM] [Info] Start training from score -3.398162\n",
      "[LightGBM] [Info] Start training from score -2.165072\n",
      "[LightGBM] [Info] Start training from score -2.962844\n",
      "[LightGBM] [Info] Start training from score -4.985128\n",
      "[LightGBM] [Info] Start training from score -1.793280\n",
      "[LightGBM] [Info] Start training from score -4.004298\n",
      "[LightGBM] [Info] Start training from score -3.850148\n",
      "[LightGBM] [Info] Start training from score -2.851619\n",
      "[LightGBM] [Info] Start training from score -2.948246\n",
      "[LightGBM] [Info] Start training from score -3.468780\n",
      "[LightGBM] [Info] Start training from score -3.571434\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 0.5333333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        7758       1.00      0.94      0.97        18\n",
      "        7762       0.51      0.44      0.47        68\n",
      "        7763       0.50      0.38      0.43        13\n",
      "        7765       0.76      0.68      0.72        50\n",
      "        7771       0.00      0.00      0.00        10\n",
      "        7800       0.48      0.62      0.55        24\n",
      "        7801       0.50      0.57      0.53        14\n",
      "        7803       1.00      0.50      0.67         6\n",
      "        7804       0.45      0.54      0.49        68\n",
      "        7821       0.18      0.50      0.27         4\n",
      "        7825       0.00      0.00      0.00         6\n",
      "        7830       0.33      0.33      0.33        15\n",
      "        7833       0.80      0.40      0.53        20\n",
      "       57663       0.47      0.80      0.59        10\n",
      "       57671       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.53       330\n",
      "   macro avg       0.52      0.51      0.50       330\n",
      "weighted avg       0.55      0.53      0.53       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = 'cleaned_final_description_not_nan.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrar clases de acc_classification con pocas muestras\n",
    "min_samples = 15  # Define el mínimo de muestras por clase\n",
    "class_counts = df['acc_classification'].value_counts()\n",
    "filtered_classes = class_counts[class_counts >= min_samples].index\n",
    "df = df[df['acc_classification'].isin(filtered_classes)]\n",
    "\n",
    "# Asegurarse de que la columna unit_total sea de tipo entero\n",
    "df['unit_total'] = df['unit_total'].astype(int)\n",
    "\n",
    "# Codificar la columna de la variable objetivo\n",
    "label_encoder_acc = LabelEncoder()\n",
    "df['acc_classification'] = label_encoder_acc.fit_transform(df['acc_classification'])\n",
    "\n",
    "# Codificar la columna classification\n",
    "label_encoder_classification = LabelEncoder()\n",
    "df['classification'] = label_encoder_classification.fit_transform(df['classification'])\n",
    "\n",
    "# Definir las características (features) y la etiqueta (label)\n",
    "X = df[['company_tid', 'classification', 'unit_total']]\n",
    "y = df['acc_classification']\n",
    "\n",
    "# Dividir el dataset en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el preprocesador usando OneHotEncoder y StandardScaler\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['company_tid', 'classification']),\n",
    "        ('num', StandardScaler(), ['unit_total'])\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline con el preprocesador y el modelo LightGBM\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgb.LGBMClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Identificar las clases presentes en y_test y y_pred\n",
    "unique_classes = sorted(set(y_test) | set(y_pred))\n",
    "target_names = [str(label_encoder_acc.inverse_transform([cls])[0]) for cls in unique_classes]\n",
    "\n",
    "# Mostrar métricas de desempeño\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
